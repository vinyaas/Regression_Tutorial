{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In corporate settings, data scientists widely use the following methods to handle multicollinearity:\n",
    "\n",
    "1. **Principal Component Analysis (PCA)**: PCA is a popular technique for reducing dimensionality while retaining most of the information in the data. It is widely used in corporate settings because it is easy to implement and interpret.\n",
    "2. **Lasso Regression**: Lasso regression is a widely used regularization technique that adds a penalty term to the loss function to shrink the coefficients of correlated variables. It is commonly used in corporate settings because it can handle high-dimensional data and is robust to multicollinearity.\n",
    "3. **Cross-validation**: Cross-validation is a widely used technique for evaluating the performance of a model on unseen data. It is commonly used in corporate settings because it provides a robust estimate of the model's performance and can help prevent overfitting.\n",
    "\n",
    "The choice of method depends on the specific problem and dataset. Here's a general outline of when and how to use each method:\n",
    "\n",
    "**Data Preprocessing**:\n",
    "\n",
    "* **Data exploration**: Use correlation matrices, scatter plots, and other visualization techniques to understand the relationships between variables.\n",
    "* **Feature selection**: Use techniques like recursive feature elimination, mutual information, or permutation importance to select the most relevant features.\n",
    "* **Data transformation**: Use log transformation or standardization to reduce multicollinearity.\n",
    "\n",
    "**Dimensionality Reduction**:\n",
    "\n",
    "* **PCA**: Use PCA when the dataset has a large number of variables and you want to reduce dimensionality while retaining most of the information in the data.\n",
    "* **t-SNE**: Use t-SNE when you want to identify clusters and patterns in the data.\n",
    "* **Autoencoders**: Use autoencoders when you want to reduce dimensionality and learn features from the data.\n",
    "\n",
    "**Regularization Techniques**:\n",
    "\n",
    "* **Lasso Regression**: Use lasso regression when you want to handle high-dimensional data and are concerned about multicollinearity.\n",
    "* **Ridge Regression**: Use ridge regression when you want to reduce the impact of multicollinearity but still want to retain all the variables.\n",
    "* **Elastic Net**: Use elastic net when you want to handle both sparse and dense data.\n",
    "\n",
    "**Model Selection**:\n",
    "\n",
    "* **Cross-validation**: Use cross-validation to evaluate the performance of a model on unseen data and prevent overfitting.\n",
    "* **Grid search**: Use grid search to find the best hyperparameters for a model.\n",
    "* **Bayesian model selection**: Use Bayesian model selection when you want to select the best model based on Bayesian principles.\n",
    "\n",
    "In terms of when to use each method, here's a general outline:\n",
    "\n",
    "1. **Data preprocessing**: Perform data preprocessing before modeling to detect and handle multicollinearity.\n",
    "2. **Dimensionality reduction**: Use dimensionality reduction techniques when the dataset has a large number of variables and you want to reduce dimensionality.\n",
    "3. **Regularization techniques**: Use regularization techniques when you want to handle high-dimensional data and are concerned about multicollinearity.\n",
    "4. **Model selection**: Use model selection techniques to choose the best model that handles multicollinearity.\n",
    "\n",
    "Here's an example of how a data scientist might use these methods in practice:\n",
    "\n",
    "1. **Load the dataset**: Load the dataset and perform data exploration to understand the relationships between variables.\n",
    "2. **Preprocess the data**: Preprocess the data by selecting the most relevant features and transforming variables to reduce multicollinearity.\n",
    "3. **Reduce dimensionality**: Use PCA to reduce dimensionality and retain most of the information in the data.\n",
    "4. **Use regularization techniques**: Use lasso regression to handle high-dimensional data and reduce the impact of multicollinearity.\n",
    "5. **Evaluate the model**: Use cross-validation to evaluate the performance of the model on unseen data and prevent overfitting.\n",
    "6. **Select the best model**: Use grid search or Bayesian model selection to select the best model that handles multicollinearity.\n",
    "\n",
    "By following this outline, data scientists can effectively handle multicollinearity in large datasets and build robust models that generalize well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Size</th>\n",
       "      <th>Bedrooms</th>\n",
       "      <th>Bathrooms</th>\n",
       "      <th>Age</th>\n",
       "      <th>Distance_to_City</th>\n",
       "      <th>Number_of_Floors</th>\n",
       "      <th>Lot_Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176446</td>\n",
       "      <td>618</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>236446</td>\n",
       "      <td>918</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>136446</td>\n",
       "      <td>318</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>296446</td>\n",
       "      <td>1218</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>206446</td>\n",
       "      <td>618</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Price  Size  Bedrooms  Bathrooms  Age  Distance_to_City  Number_of_Floors  \\\n",
       "0  176446   618         3          1    6                 7                 3   \n",
       "1  236446   918         4          2   12                 7                 4   \n",
       "2  136446   318         2          4    5                 7                 2   \n",
       "3  296446  1218         5          2    5                 4                 4   \n",
       "4  206446   618         3          2    4                 6                 3   \n",
       "\n",
       "   Lot_Size  \n",
       "0      5991  \n",
       "1      6389  \n",
       "2      5232  \n",
       "3      6338  \n",
       "4      6678  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "data = {\n",
    "    'Price': [250000, 300000, 200000, 350000, 280000, 320000, 240000, 380000, 260000, 290000],\n",
    "    'Size': [1500, 2000, 1200, 2500, 1800, 2200, 1400, 2800, 1600, 2000],\n",
    "    'Bedrooms': [3, 4, 2, 5, 3, 4, 2, 5, 3, 4],\n",
    "    'Bathrooms': [np.random.randint(1, 5) for _ in range(10)],\n",
    "    'Age': [np.random.randint(1, 20) for _ in range(10)],\n",
    "    'Distance_to_City': [np.random.randint(1, 10) for _ in range(10)],\n",
    "    'Number_of_Floors': [np.random.randint(1, 5) for _ in range(10)],\n",
    "    'Lot_Size': [np.random.randint(4000, 8000) for _ in range(10)]\n",
    "}\n",
    "\n",
    "# Create a pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce high correlation between Price, Size, and Bedrooms\n",
    "df['Price'] = df['Size'] * 100 + df['Bedrooms'] * 10000 + np.random.randint(-10000, 10000)\n",
    "df['Size'] = df['Bedrooms'] * 300 + np.random.randint(-500, 500)\n",
    "\n",
    "# Print the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a detailed explanation of the parameters of PCA and everything related to it:\n",
    "\n",
    "**PCA Parameters**\n",
    "\n",
    "1. **n_components**: This parameter determines the number of principal components to retain. It can be an integer or a float.\n",
    "\t* If it's an integer, it specifies the number of components to retain.\n",
    "\t* If it's a float, it specifies the percentage of variance to retain.\n",
    "2. **copy**: This parameter determines whether to copy the input data or not. If it's `True`, the input data is copied, and if it's `False`, the input data is modified in place.\n",
    "3. **whiten**: This parameter determines whether to whiten the data or not. If it's `True`, the data is whitened, and if it's `False`, the data is not whitened.\n",
    "4. **svd_solver**: This parameter determines the solver to use for the SVD decomposition. The available options are:\n",
    "\t* `auto`: The solver is automatically chosen based on the size of the input data.\n",
    "\t* `full`: The full SVD decomposition is used.\n",
    "\t* `arpack`: The ARPACK solver is used.\n",
    "\t* `randomized`: The randomized SVD solver is used.\n",
    "5. **tol**: This parameter determines the tolerance for the SVD decomposition. It's used to determine when to stop the iteration.\n",
    "6. **iterated_power**: This parameter determines the number of iterations for the SVD decomposition.\n",
    "7. **random_state**: This parameter determines the random seed for the SVD decomposition.\n",
    "\n",
    "**PCA Methods**\n",
    "\n",
    "1. **fit**: This method fits the PCA model to the input data.\n",
    "2. **transform**: This method transforms the input data into the new coordinate system.\n",
    "3. **fit_transform**: This method fits the PCA model to the input data and transforms it into the new coordinate system.\n",
    "4. **inverse_transform**: This method transforms the data back into the original coordinate system.\n",
    "5. **get_covariance**: This method returns the covariance matrix of the input data.\n",
    "6. **get_precision**: This method returns the precision matrix of the input data.\n",
    "\n",
    "**PCA Attributes**\n",
    "\n",
    "1. **components_**: This attribute returns the principal components of the input data.\n",
    "2. **explained_variance_ratio_**: This attribute returns the explained variance ratio of each principal component.\n",
    "3. **singular_values_**: This attribute returns the singular values of the input data.\n",
    "4. **mean_**: This attribute returns the mean of the input data.\n",
    "5. **n_features_**: This attribute returns the number of features in the input data.\n",
    "6. **n_samples_**: This attribute returns the number of samples in the input data.\n",
    "\n",
    "**PCA Example**\n",
    "\n",
    "Here's an example of using PCA in Python:\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Create a PCA object with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA model to the data\n",
    "pca.fit(X)\n",
    "\n",
    "# Transform the data into the new coordinate system\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Print the principal components\n",
    "print(pca.components_)\n",
    "\n",
    "# Plot the data in the new coordinate system\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n",
    "```\n",
    "This example loads the iris dataset, creates a PCA object with 2 components, fits the PCA model to the data, transforms the data into the new coordinate system, and plots the data in the new coordinate system.\n",
    "\n",
    "**PCA Advantages**\n",
    "\n",
    "1. **Dimensionality reduction**: PCA can reduce the dimensionality of the input data, making it easier to visualize and analyze.\n",
    "2. **Noise reduction**: PCA can reduce the noise in the input data, making it easier to identify patterns and relationships.\n",
    "3. **Feature extraction**: PCA can extract the most important features from the input data, making it easier to identify the underlying structure of the data.\n",
    "4. **Data visualization**: PCA can be used to visualize the input data in a lower-dimensional space, making it easier to understand the relationships between the variables.\n",
    "\n",
    "**PCA Disadvantages**\n",
    "\n",
    "1. **Assumes linearity**: PCA assumes that the relationships between the variables are linear, which may not always be the case.\n",
    "2. **Sensitive to outliers**: PCA can be sensitive to outliers in the input data, which can affect the accuracy of the results.\n",
    "3. **Not suitable for non-normal data**: PCA assumes that the input data is normally distributed, which may not always be the case.\n",
    "4. **Can be computationally expensive**: PCA can be computationally expensive, especially for large datasets.\n",
    "\n",
    "**PCA Applications**\n",
    "\n",
    "1. **Data visualization**: PCA can be used to visualize the input data in a lower-dimensional space, making it easier to understand the relationships between the variables.\n",
    "2. **Feature extraction**: PCA can be used to extract the most important features from the input data, making it easier to identify the underlying structure of the data.\n",
    "3. **Noise reduction**: PCA can be used to reduce the noise in the input data, making it easier to identify patterns and relationships.\n",
    "4. **Dimensionality reduction**: PCA can be used to reduce the dimensionality of the input data, making it easier to analyze and visualize.\n",
    "5. **Anomaly detection**: PCA can be used to detect anomalies in the input data, making it easier to identify unusual patterns and relationships.\n",
    "6. **Clustering**: PCA can be used to cluster the input data, making it easier to identify groups and patterns in the data.\n",
    "7. **Regression**: PCA can be used to improve the accuracy of regression models by reducing the dimensionality of the input data and removing noise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.55818964 1.54206507]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.48894738, -1.68224541],\n",
       "       [ 1.16868454, -0.7538723 ],\n",
       "       [-2.26219909, -0.11600993],\n",
       "       [ 1.81355444, -0.3249741 ],\n",
       "       [-0.23621337, -0.52777293],\n",
       "       [ 0.04671478,  2.84821574],\n",
       "       [-3.5861117 , -0.42339797],\n",
       "       [ 2.6442198 , -0.61727154],\n",
       "       [-0.4709771 ,  1.0578455 ],\n",
       "       [ 1.37127507,  0.53948293]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x = df.drop('Price' , axis = 1)\n",
    "\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "\n",
    "pca = PCA(n_components= 2)\n",
    "x_pca = pca.fit_transform(x_scaled)\n",
    "\n",
    "print(pca.explained_variance_)\n",
    "x_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's consider a real-life example to illustrate how multicollinearity can affect model performance.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Suppose we are a marketing team for a company that sells cars, and we want to build a model to predict the price of a car based on its features. We collect data on the following variables:\n",
    "\n",
    "* Price (response variable)\n",
    "* Engine size (predictor variable)\n",
    "* Horsepower (predictor variable)\n",
    "* Number of cylinders (predictor variable)\n",
    "* Weight (predictor variable)\n",
    "\n",
    "We collect data on 100 cars and build a multiple linear regression model to predict the price of a car based on these features. The data looks like this:\n",
    "\n",
    "| Car | Price | Engine Size | Horsepower | Number of Cylinders | Weight |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 1 | $20,000 | 2.0L | 150HP | 4 | 1500kg |\n",
    "| 2 | $25,000 | 2.5L | 200HP | 6 | 1700kg |\n",
    "| 3 | $30,000 | 3.0L | 250HP | 8 | 2000kg |\n",
    "|... |... |... |... |... |... |\n",
    "| 100 | $40,000 | 4.0L | 350HP | 12 | 2500kg |\n",
    "\n",
    "**Multicollinearity:**\n",
    "\n",
    "Upon analyzing the data, we notice that the variables \"Engine size\", \"Horsepower\", and \"Number of cylinders\" are highly correlated with each other. For example:\n",
    "\n",
    "* Cars with larger engines tend to have more horsepower and more cylinders.\n",
    "* Cars with more horsepower tend to have larger engines and more cylinders.\n",
    "\n",
    "This means that these variables are not independent of each other, and we have multicollinearity in our data.\n",
    "\n",
    "**Effects of Multicollinearity:**\n",
    "\n",
    "Because of multicollinearity, our model may produce unstable coefficients and poor predictions. For example:\n",
    "\n",
    "* The coefficient for \"Engine size\" may be very large and positive, indicating that larger engines are associated with higher prices. However, this may be due to the fact that larger engines are also associated with more horsepower and more cylinders, which are also correlated with higher prices.\n",
    "* The coefficient for \"Horsepower\" may be very small and negative, indicating that more horsepower is associated with lower prices. However, this may be due to the fact that more horsepower is also associated with larger engines and more cylinders, which are correlated with higher prices.\n",
    "\n",
    "As a result, our model may produce poor predictions and may not accurately capture the relationships between the variables.\n",
    "\n",
    "**Consequences:**\n",
    "\n",
    "The consequences of multicollinearity in this example are:\n",
    "\n",
    "* Our model may not accurately predict the price of a car based on its features.\n",
    "* We may make incorrect conclusions about the relationships between the variables, such as thinking that larger engines are associated with higher prices when in fact it's the combination of larger engines, more horsepower, and more cylinders that's driving the price.\n",
    "* We may overfit the model to the training data, which means that it will perform poorly on new, unseen data.\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "To address multicollinearity, we could:\n",
    "\n",
    "* Remove one or more of the correlated variables from the model.\n",
    "* Use dimensionality reduction techniques, such as principal component analysis (PCA), to reduce the number of variables.\n",
    "* Use regularization techniques, such as Lasso or Ridge regression, to reduce the impact of multicollinearity.\n",
    "* Collect more data to increase the sample size and reduce the correlation between variables.\n",
    "\n",
    "By addressing multicollinearity, we can build a more robust and accurate model that provides valuable insights into the relationships between the variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
