{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of using Generalized Additive Models (GAM) with a sample dataset in Python. We'll use the `pygam` library, which provides an implementation of GAM.\n",
    "\n",
    "**Dataset:**\n",
    "We'll use the `mtcars` dataset, which is a classic dataset in the R language. It contains information about various car models, including their mileage, weight, and horsepower.\n",
    "\n",
    "**Goal:**\n",
    "Our goal is to predict the mileage of a car based on its weight and horsepower using a GAM.\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pygam import LinearGAM, s\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/datasets/mtcars.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Convert the dataset to a Pandas dataframe\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# Define the features and target variable\n",
    "X = df[['wt', 'hp']]\n",
    "y = df['mpg']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a GAM model\n",
    "gam = LinearGAM(s(0) + s(1)).fit(X_train, y_train)\n",
    "\n",
    "# Print the summary of the GAM model\n",
    "print(gam.summary())\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gam.predict(X_test)\n",
    "\n",
    "# Evaluate the model using mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "\n",
    "# Plot the partial dependence plots for each feature\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "gam.partial_dependence('wt', ax=axs[0])\n",
    "gam.partial_dependence('hp', ax=axs[1])\n",
    "plt.show()\n",
    "```\n",
    "**Explanation:**\n",
    "\n",
    "1. We first load the necessary libraries, including `pandas` for data manipulation, `numpy` for numerical computations, and `pygam` for GAM implementation.\n",
    "2. We load the `mtcars` dataset from a CSV file and convert it to a Pandas dataframe.\n",
    "3. We define the features (`wt` and `hp`) and the target variable (`mpg`).\n",
    "4. We split the dataset into training and testing sets using `train_test_split`.\n",
    "5. We create a GAM model using `LinearGAM` from `pygam`. We specify the features using the `s` function, which represents a smooth term. In this case, we use `s(0)` for the first feature (`wt`) and `s(1)` for the second feature (`hp`).\n",
    "6. We fit the GAM model to the training data using the `fit` method.\n",
    "7. We print the summary of the GAM model using the `summary` method.\n",
    "8. We make predictions on the test set using the `predict` method.\n",
    "9. We evaluate the model using mean squared error (MSE) and print the result.\n",
    "10. We plot the partial dependence plots for each feature using the `partial_dependence` method. These plots show the relationship between each feature and the predicted response variable.\n",
    "\n",
    "**Hyperparameter Tuning:**\n",
    "To tune the hyperparameters of the GAM model, we can use a grid search approach. We can define a range of values for the hyperparameters and evaluate the model's performance for each combination of hyperparameters. Here's an example:\n",
    "```python\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'lam': [0.1, 0.5, 1, 5],\n",
    "    'n_splines': [10, 20, 30]\n",
    "}\n",
    "\n",
    "# Initialize the best parameters and best score\n",
    "best_params = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# Perform grid search\n",
    "for lam in param_grid['lam']:\n",
    "    for n_splines in param_grid['n_splines']:\n",
    "        gam = LinearGAM(s(0, lam=lam, n_splines=n_splines) + s(1, lam=lam, n_splines=n_splines)).fit(X_train, y_train)\n",
    "        y_pred = gam.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        if mse < best_score:\n",
    "            best_score = mse\n",
    "            best_params = {'lam': lam, 'n_splines': n_splines}\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Score: {best_score:.2f}\")\n",
    "```\n",
    "In this example, we define a grid of values for the `lam` and `n_splines` hyperparameters. We then perform a grid search by iterating over each combination of hyperparameters and evaluating the model's performance using MSE. We keep track of the best parameters and best score, and print the results at the end.\n",
    "\n",
    "**Working of GAM Algorithm:**\n",
    "\n",
    "GAM is a type of additive model that uses a sum of smooth functions to model the relationship between the features and the response variable. The smooth functions are typically represented using basis functions, such as splines or polynomials.\n",
    "\n",
    "The GAM algorithm works as follows:\n",
    "\n",
    "1. **Basis function expansion**: Each feature is expanded into a set of basis functions, which are used to represent the smooth functions.\n",
    "2. **Linear combination**: The basis functions are combined linearly to form the predicted response variable.\n",
    "3. **Smoothness penalty**: A penalty term is added to the loss function to encourage smoothness in the predicted response variable.\n",
    "4. **Optimization**: The model is optimized using a iterative algorithm, such as gradient descent or Newton's method, to minimize the loss function.\n",
    "\n",
    "The GAM algorithm has several advantages, including:\n",
    "\n",
    "* **Flexibility**: GAM can model complex relationships between features and response variables.\n",
    "* **Interpretability**: The partial dependence plots provide a clear understanding of the relationship between each feature and the predicted response variable.\n",
    "* **Robustness**: GAM is robust to outliers and non-normality in the data.\n",
    "\n",
    "However, GAM also has some limitations, including:\n",
    "\n",
    "* **Computational complexity**: GAM can be computationally expensive, especially for large datasets.\n",
    "* **Overfitting**: GAM can suffer from overfitting, especially when the number of basis functions is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here are some alternative ways for hyperparameter tuning for Generalized Additive Models (GAM):\n",
    "\n",
    "1. **Grid Search**: This method involves defining a range of possible values for each hyperparameter and training the model on each combination of hyperparameters.\n",
    "2. **Random Search**: This method involves randomly sampling the hyperparameter space and training the model on each sampled combination of hyperparameters.\n",
    "3. **Bayesian Optimization**: This method uses a probabilistic approach to search for the optimal hyperparameters. It uses a Gaussian process to model the relationship between the hyperparameters and the performance metric, and then uses this model to select the next set of hyperparameters to try.\n",
    "4. **Gradient-Based Optimization**: This method uses gradient-based optimization algorithms, such as gradient descent or gradient ascent, to optimize the hyperparameters.\n",
    "5. **Cross-Validation**: This method involves splitting the data into multiple folds and training the model on each fold with a different set of hyperparameters. The hyperparameters that result in the best performance across all folds are selected.\n",
    "\n",
    "Here is an example of how you can use Grid Search for hyperparameter tuning for GAM:\n",
    "```python\n",
    "from pygam import LinearGAM, s\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_grid = {\n",
    "    'lam': [0.1, 0.5, 1, 5],\n",
    "    'n_splines': [10, 20, 30]\n",
    "}\n",
    "\n",
    "# Initialize the best parameters and best score\n",
    "best_params = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# Perform grid search\n",
    "for lam in param_grid['lam']:\n",
    "    for n_splines in param_grid['n_splines']:\n",
    "        gam = LinearGAM(s(0, lam=lam, n_splines=n_splines) + s(1, lam=lam, n_splines=n_splines)).fit(X_train, y_train)\n",
    "        y_pred = gam.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        if mse < best_score:\n",
    "            best_score = mse\n",
    "            best_params = {'lam': lam, 'n_splines': n_splines}\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", best_params)\n",
    "print(\"Best Score: \", best_score)\n",
    "```\n",
    "And here is an example of how you can use Random Search for hyperparameter tuning for GAM:\n",
    "```python\n",
    "from pygam import LinearGAM, s\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_grid = {\n",
    "    'lam': [0.1, 0.5, 1, 5],\n",
    "    'n_splines': [10, 20, 30]\n",
    "}\n",
    "\n",
    "# Initialize the best parameters and best score\n",
    "best_params = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# Perform random search\n",
    "for _ in range(10):\n",
    "    lam = np.random.choice(param_grid['lam'])\n",
    "    n_splines = np.random.choice(param_grid['n_splines'])\n",
    "    gam = LinearGAM(s(0, lam=lam, n_splines=n_splines) + s(1, lam=lam, n_splines=n_splines)).fit(X_train, y_train)\n",
    "    y_pred = gam.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    if mse < best_score:\n",
    "        best_score = mse\n",
    "        best_params = {'lam': lam, 'n_splines': n_splines}\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", best_params)\n",
    "print(\"Best Score: \", best_score)\n",
    "```\n",
    "And here is an example of how you can use Bayesian Optimization for hyperparameter tuning for GAM:\n",
    "```python\n",
    "from pygam import LinearGAM, s\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# Define the hyperparameter space\n",
    "search_space = [\n",
    "    Real(0.1, 5, name='lam'),\n",
    "    Integer(10, 30, name='n_splines')\n",
    "]\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    lam, n_splines = params\n",
    "    gam = LinearGAM(s(0, lam=lam, n_splines=n_splines) + s(1, lam=lam, n_splines=n_splines)).fit(X_train, y_train)\n",
    "    y_pred = gam.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "res_gp = gp_minimize(objective, search_space, n_calls=10, random_state=42)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", dict(zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
