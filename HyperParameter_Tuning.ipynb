{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning in Regression**\n",
    "=====================================\n",
    "\n",
    "Hyperparameter tuning is a crucial step in machine learning, including regression models. Hyperparameters are parameters that are set before training a model, and they can significantly impact the model's performance. In this section, we will discuss hyperparameter tuning in regression models in detail.\n",
    "\n",
    "**What are Hyperparameters in Regression?**\n",
    "----------------------------------------\n",
    "\n",
    "In regression models, hyperparameters are the parameters that are set before training the model. Some common hyperparameters in regression models include:\n",
    "\n",
    "* **Regularization strength** (α): This hyperparameter controls the amount of regularization applied to the model. Regularization helps prevent overfitting by adding a penalty term to the loss function.\n",
    "* **Learning rate** (η): This hyperparameter controls how quickly the model learns from the data. A high learning rate can lead to fast convergence, but may also cause the model to overshoot the optimal solution.\n",
    "* **Number of iterations** (n_iter): This hyperparameter controls the number of times the model is trained on the data.\n",
    "* **Polynomial degree** (d): This hyperparameter controls the degree of the polynomial used in polynomial regression.\n",
    "* **Number of hidden layers** (n_layers): This hyperparameter controls the number of hidden layers in a neural network regression model.\n",
    "* **Number of units in each layer** (n_units): This hyperparameter controls the number of units (neurons) in each hidden layer.\n",
    "\n",
    "**Why is Hyperparameter Tuning Important?**\n",
    "-----------------------------------------\n",
    "\n",
    "Hyperparameter tuning is important because it can significantly impact the performance of a regression model. A well-tuned model can:\n",
    "\n",
    "* **Improve accuracy**: Hyperparameter tuning can help improve the accuracy of a regression model by finding the optimal combination of hyperparameters.\n",
    "* **Prevent overfitting**: Hyperparameter tuning can help prevent overfitting by finding the optimal regularization strength and learning rate.\n",
    "* **Reduce computational cost**: Hyperparameter tuning can help reduce the computational cost of training a model by finding the optimal number of iterations and hidden layers.\n",
    "\n",
    "**Hyperparameter Tuning Techniques**\n",
    "----------------------------------\n",
    "\n",
    "There are several hyperparameter tuning techniques, including:\n",
    "\n",
    "### 1. **Grid Search**\n",
    "\n",
    "Grid search is a brute-force approach to hyperparameter tuning. It involves defining a grid of possible hyperparameter values and evaluating the model's performance for each combination of hyperparameters.\n",
    "\n",
    "**Example Code:**\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 1, 10],\n",
    "   'max_iter': [100, 1000, 10000]\n",
    "}\n",
    "\n",
    "# Define the model\n",
    "model = Ridge()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the best score\n",
    "print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n",
    "```\n",
    "\n",
    "### 2. **Random Search**\n",
    "\n",
    "Random search is similar to grid search, but it involves randomly sampling the hyperparameter space instead of defining a grid.\n",
    "\n",
    "**Example Code:**\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the hyperparameter distribution\n",
    "param_dist = {\n",
    "    'alpha': randint(0.1, 10),\n",
    "   'max_iter': randint(100, 10000)\n",
    "}\n",
    "\n",
    "# Define the model\n",
    "model = Ridge()\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, cv=5, n_iter=10)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the best score\n",
    "print(\"Best Hyperparameters: \", random_search.best_params_)\n",
    "print(\"Best Score: \", random_search.best_score_)\n",
    "```\n",
    "\n",
    "### 3. **Bayesian Optimization**\n",
    "\n",
    "Bayesian optimization is a more efficient approach to hyperparameter tuning. It involves using a probabilistic model to search the hyperparameter space.\n",
    "\n",
    "**Example Code:**\n",
    "```python\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Define the hyperparameter space\n",
    "search_space = {\n",
    "    'alpha': Real(0.1, 10, \"uniform\"),\n",
    "   'max_iter': Integer(100, 10000, \"uniform\")\n",
    "}\n",
    "\n",
    "# Define the model\n",
    "model = Ridge()\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "bayes_search = BayesSearchCV(model, search_space, cv=5, n_iter=10)\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the best score\n",
    "print(\"Best Hyperparameters: \", bayes_search.best_params_)\n",
    "print(\"Best Score: \", bayes_search.best_score_)\n",
    "```\n",
    "\n",
    "### 4. **Gradient-Based Optimization**\n",
    "\n",
    "Gradient-based optimization involves using gradient descent to optimize the hyperparameters.\n",
    "\n",
    "**Example Code:** \n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 10)  # input layer (10) -> hidden layer (10)\n",
    "        self.fc2 = nn.Linear(10, 1)  # hidden layer (10) -> output layer (1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))      # activation function for hidden layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = RegressionModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Perform gradient-based optimization\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 100, loss.item()))\n",
    "```\n",
    "\n",
    "**Best Practices for Hyperparameter Tuning**\n",
    "-----------------------------------------\n",
    "\n",
    "Here are some best practices for hyperparameter tuning:\n",
    "\n",
    "* **Use a suitable hyperparameter tuning technique**: Choose a hyperparameter tuning technique that is suitable for your problem and dataset.\n",
    "* **Use a suitable hyperparameter space**: Define a hyperparameter space that is suitable for your problem and dataset.\n",
    "* **Use cross-validation**: Use cross-validation to evaluate the model's performance and prevent overfitting.\n",
    "* **Monitor the model's performance**: Monitor the model's performance during hyperparameter tuning and stop the process when the model's performance stops improving.\n",
    "* **Use a suitable evaluation metric**: Use a suitable evaluation metric to evaluate the model's performance.\n",
    "\n",
    "By following these best practices and using a suitable hyperparameter tuning technique, you can find the optimal hyperparameters for your regression model and improve its performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the concept of alpha (regularization strength) in regression in simple terms.\n",
    "\n",
    "**What is alpha?**\n",
    "\n",
    "In regression analysis, alpha (α) is a parameter that controls the strength of regularization. Regularization is a technique used to prevent overfitting, which occurs when a model is too complex and fits the noise in the training data rather than the underlying patterns.\n",
    "\n",
    "**What does alpha do?**\n",
    "\n",
    "Alpha determines how much penalty is applied to the model for having large coefficients (weights) for the features. In other words, alpha controls how much the model is allowed to \"stretch\" or \"shrink\" the coefficients to fit the data.\n",
    "\n",
    "**How does alpha work?**\n",
    "\n",
    "Imagine you have a regression model with several features, and each feature has a coefficient (weight) that determines its importance in the model. If alpha is:\n",
    "\n",
    "* **Low (e.g., 0.01)**: The model is allowed to have large coefficients, which means it can fit the data very closely. This can lead to overfitting, where the model becomes too specialized to the training data and doesn't generalize well to new data.\n",
    "* **High (e.g., 1.0)**: The model is penalized for having large coefficients, which means it will try to reduce the coefficients to smaller values. This can lead to underfitting, where the model is too simple and doesn't capture the underlying patterns in the data.\n",
    "* **Medium (e.g., 0.1)**: The model finds a balance between fitting the data closely and avoiding overfitting.\n",
    "\n",
    "**Effects of alpha on the model**\n",
    "\n",
    "By adjusting alpha, you can control the following aspects of the model:\n",
    "\n",
    "1. **Model complexity**: Higher alpha values result in simpler models with fewer features, while lower alpha values result in more complex models with more features.\n",
    "2. **Coefficient values**: Higher alpha values shrink the coefficients towards zero, while lower alpha values allow larger coefficients.\n",
    "3. **Overfitting**: Higher alpha values reduce overfitting, while lower alpha values increase the risk of overfitting.\n",
    "4. **Prediction accuracy**: The optimal alpha value depends on the specific problem and dataset. A good alpha value can improve prediction accuracy, while a poor choice can lead to suboptimal performance.\n",
    "\n",
    "**Common values for alpha**\n",
    "\n",
    "The choice of alpha depends on the specific problem and dataset. Here are some common values:\n",
    "\n",
    "* L1 regularization (Lasso): 0.01, 0.1, 1.0\n",
    "* L2 regularization (Ridge): 0.1, 1.0, 10.0\n",
    "* Elastic Net regularization: 0.01, 0.1, 1.0 (for both L1 and L2 components)\n",
    "\n",
    "Let's use a simple example to illustrate the concept of alpha (regularization strength) and its effect on model fitting.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Suppose we have a simple linear regression model that tries to predict the price of a house based on its size (in square feet). We have a dataset with 10 houses, each with a size and a price.\n",
    "\n",
    "| House Size (sq ft) | Price |\n",
    "| --- | --- |\n",
    "| 1000 | 200,000 |\n",
    "| 1200 | 250,000 |\n",
    "| 1500 | 300,000 |\n",
    "| 1800 | 350,000 |\n",
    "| 2000 | 400,000 |\n",
    "| 2200 | 450,000 |\n",
    "| 2500 | 500,000 |\n",
    "| 2800 | 550,000 |\n",
    "| 3000 | 600,000 |\n",
    "| 3200 | 650,000 |\n",
    "\n",
    "We'll use a simple linear regression model to fit the data: `Price = β0 + β1 * Size`\n",
    "\n",
    "**Low alpha (e.g., 0.01)**\n",
    "\n",
    "When alpha is low (e.g., 0.01), the model is allowed to have large coefficients (β1) to fit the data very closely. This means the model will try to match the data points as closely as possible, even if it means creating a complex model.\n",
    "\n",
    "In this case, the model might fit the data like this:\n",
    "\n",
    "`Price = 100,000 + 250 * Size`\n",
    "\n",
    "This model fits the data very closely, but it's also very sensitive to the individual data points. If we plot the data and the model, we might see a curve that passes very close to each data point.\n",
    "\n",
    "**Problem:** This model is prone to overfitting, which means it might not generalize well to new data. If we were to use this model to predict the price of a new house with a size of 3500 sq ft, the model might predict a price that's way off (e.g., $1,000,000).\n",
    "\n",
    "**High alpha (e.g., 1.0)**\n",
    "\n",
    "When alpha is high (e.g., 1.0), the model is penalized for having large coefficients (β1). This means the model will try to reduce the coefficients to smaller values, resulting in a simpler model.\n",
    "\n",
    "In this case, the model might fit the data like this:\n",
    "\n",
    "`Price = 150,000 + 100 * Size`\n",
    "\n",
    "This model is much simpler and less sensitive to individual data points. However, it might not capture the underlying pattern in the data as well.\n",
    "\n",
    "**Problem:** This model is prone to underfitting, which means it might not capture the underlying patterns in the data. If we plot the data and the model, we might see a straight line that doesn't fit the data very well.\n",
    "\n",
    "**Medium alpha (e.g., 0.1)**\n",
    "\n",
    "When alpha is medium (e.g., 0.1), the model finds a balance between fitting the data closely and avoiding overfitting.\n",
    "\n",
    "In this case, the model might fit the data like this:\n",
    "\n",
    "`Price = 120,000 + 150 * Size`\n",
    "\n",
    "This model is a good balance between the low-alpha and high-alpha models. It fits the data reasonably well, but it's not too sensitive to individual data points.\n",
    "\n",
    "**Solution:** This model is a good compromise between overfitting and underfitting. If we were to use this model to predict the price of a new house with a size of 3500 sq ft, the model might predict a reasonable price (e.g., $525,000).\n",
    "\n",
    "In summary:\n",
    "\n",
    "* Low alpha (e.g., 0.01): Model fits data very closely, but is prone to overfitting.\n",
    "* High alpha (e.g., 1.0): Model is simple, but might not capture underlying patterns (underfitting).\n",
    "* Medium alpha (e.g., 0.1): Model finds a balance between fitting data closely and avoiding overfitting.\n",
    "\n",
    "I hope this example helps illustrate the concept of alpha (regularization strength) and its effect on model fitting!\n",
    "\n",
    "In summary, alpha (regularization strength) is a parameter that controls the trade-off between model complexity and overfitting in regression analysis. By adjusting alpha, you can influence the model's coefficients, complexity, and prediction accuracy. The optimal alpha value depends on the specific problem and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
