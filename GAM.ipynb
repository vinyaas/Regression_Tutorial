{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAM, or Generalized Additive Model, is a type of regression analysis that extends traditional linear regression by allowing for non-linear relationships between the predictors and the response variable.\n",
    "\n",
    "**What is a Generalized Additive Model?**\n",
    "\n",
    "In traditional linear regression, the relationship between the predictors (X) and the response variable (y) is modeled using a linear equation:\n",
    "\n",
    "y = β0 + β1X1 + β2X2 + … + βnXn + ε\n",
    "\n",
    "where β0 is the intercept, β1, β2, …, βn are the coefficients, X1, X2, …, Xn are the predictors, and ε is the error term.\n",
    "\n",
    "In contrast, a Generalized Additive Model replaces the linear equation with a sum of smooth functions, one for each predictor:\n",
    "\n",
    "y = β0 + f1(X1) + f2(X2) + … + fn(Xn) + ε\n",
    "\n",
    "where f1, f2, …, fn are smooth functions, such as cubic splines or loess curves, that capture non-linear relationships between each predictor and the response variable.\n",
    "\n",
    "**Key Features of GAM**\n",
    "\n",
    "1. **Non-linear relationships**: GAM allows for non-linear relationships between predictors and the response variable, which can be useful when the relationships are complex or non-monotonic.\n",
    "2. **Additivity**: The model assumes that the effects of each predictor are additive, meaning that the effect of one predictor does not depend on the values of other predictors.\n",
    "3. **Smooth functions**: GAM uses smooth functions to model the relationships, which can be interpreted as a non-parametric approach.\n",
    "4. **Flexibility**: GAM can handle a wide range of response distributions, including Gaussian, binomial, Poisson, and others.\n",
    "\n",
    "**Advantages of GAM**\n",
    "\n",
    "1. **Improved fit**: GAM can provide a better fit to the data than traditional linear regression, especially when the relationships are non-linear.\n",
    "2. **Interpretability**: The smooth functions used in GAM can provide insights into the relationships between predictors and the response variable.\n",
    "3. **Flexibility**: GAM can handle a wide range of data types and response distributions.\n",
    "\n",
    "**Common Applications of GAM**\n",
    "\n",
    "1. **Ecology**: GAM is widely used in ecology to model species distributions, abundance, and community composition.\n",
    "2. **Epidemiology**: GAM is used to model disease incidence and prevalence, and to identify risk factors.\n",
    "3. **Finance**: GAM is used to model stock prices, credit risk, and portfolio optimization.\n",
    "4. **Marketing**: GAM is used to model customer behavior, demand, and market trends.\n",
    "\n",
    "**Software for GAM**\n",
    "\n",
    "GAM can be implemented in various software packages, including:\n",
    "\n",
    "1. **R**: The `mgcv` package in R provides a comprehensive implementation of GAM.\n",
    "2. **Python**: The `pygam` package in Python provides a Python implementation of GAM.\n",
    "3. **SAS**: The `PROC GAM` procedure in SAS provides a implementation of GAM.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a complete example of using Generalized Additive Models (GAM) in Python:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pygam import LinearGAM, s, f\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate a sample dataset\n",
    "np.random.seed(0)\n",
    "size = np.random.uniform(1000, 5000, 1000)\n",
    "bedrooms = np.random.uniform(2, 6, 1000)\n",
    "distance = np.random.uniform(1, 10, 1000)\n",
    "price = 200000 + 50 * size + 20000 * bedrooms - 1000 * distance + np.random.normal(0, 10000, 1000)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'price': price,'size': size, 'bedrooms': bedrooms, 'distance': distance})\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[['size', 'bedrooms', 'distance']]\n",
    "y = df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Fit a Generalized Additive Model\n",
    "gam_model = LinearGAM(s(0) + s(1) + s(2))\n",
    "gam_model.fit(X_train, y_train)\n",
    "y_pred_gam = gam_model.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "mse_gam = mean_squared_error(y_test, y_pred_gam)\n",
    "print(f\"Linear Regression MSE: {mse_lr:.2f}\")\n",
    "print(f\"Generalized Additive Model MSE: {mse_gam:.2f}\")\n",
    "\n",
    "# Plot the partial dependence plots for the GAM\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(gam_model.partial_dependence(0, X_train), label='Partial dependence')\n",
    "plt.xlabel('Size')\n",
    "plt.ylabel('Partial dependence')\n",
    "plt.title('Size')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(gam_model.partial_dependence(1, X_train), label='Partial dependence')\n",
    "plt.xlabel('Bedrooms')\n",
    "plt.ylabel('Partial dependence')\n",
    "plt.title('Bedrooms')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(gam_model.partial_dependence(2, X_train), label='Partial dependence')\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Partial dependence')\n",
    "plt.title('Distance')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This code:\n",
    "\n",
    "1.  Generates a sample dataset of house prices and their characteristics (size, number of bedrooms, and distance from the city center).\n",
    "2.  Splits the dataset into training and testing sets.\n",
    "3.  Fits a linear regression model and a Generalized Additive Model (GAM) to the training data.\n",
    "4.  Evaluates the performance of both models using the mean squared error (MSE) metric.\n",
    "5.  Plots the partial dependence plots for the GAM to visualize the relationships between each predictor and the response variable.\n",
    "\n",
    "The GAM can capture non-linear relationships between the predictors and the response variable, which can lead to better predictions and insights into the data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a real-life dataset that makes sense and is easy to understand.\n",
    "\n",
    "**Dataset:**\n",
    "\n",
    "Suppose we have a dataset of house prices in a city, with the following columns:\n",
    "\n",
    "1. **Price**: The price of the house\n",
    "2. **Size**: The size of the house in square feet\n",
    "3. **Bedrooms**: The number of bedrooms in the house\n",
    "4. **Distance**: The distance from the city center in miles\n",
    "\n",
    "This dataset is a good example of a regression problem, where we want to predict the price of a house based on its characteristics.\n",
    "\n",
    "**Non-linear relationships:**\n",
    "\n",
    "Let's analyze the relationships between the independent variables and the target variable (Price).\n",
    "\n",
    "*   **Size and Price:** The relationship between size and price is likely to be non-linear. For example, a house with 1000 square feet may cost more than a house with 500 square feet, but a house with 2000 square feet may not cost twice as much as a house with 1000 square feet. This is an example of a non-linear, quadratic relationship.\n",
    "*   **Bedrooms and Price:** The relationship between the number of bedrooms and price is also non-linear. For example, a house with 3 bedrooms may cost more than a house with 2 bedrooms, but a house with 5 bedrooms may not cost twice as much as a house with 3 bedrooms. This is an example of a non-linear, sigmoidal relationship.\n",
    "*   **Distance and Price:** The relationship between distance from the city center and price is likely to be non-linear. For example, a house that is 1 mile from the city center may cost more than a house that is 5 miles from the city center, but a house that is 10 miles from the city center may not cost less than a house that is 5 miles from the city center. This is an example of a non-linear, exponential relationship.\n",
    "\n",
    "**Interactions between independent variables:**\n",
    "\n",
    "There are also interactions between the independent variables that can affect the price of a house. For example:\n",
    "\n",
    "*   **Size and Bedrooms:** The effect of size on price may depend on the number of bedrooms. For example, a house with 3 bedrooms and 2000 square feet may cost more than a house with 2 bedrooms and 2000 square feet.\n",
    "*   **Distance and Size:** The effect of distance on price may depend on the size of the house. For example, a large house that is far from the city center may be cheaper than a small house that is close to the city center.\n",
    "\n",
    "**GAMs for interpretation:**\n",
    "\n",
    "GAMs are useful for interpreting these non-linear relationships and interactions between independent variables. By using smooth functions to model the relationships between each independent variable and the target variable, GAMs can capture the non-linear relationships and interactions that are present in the data.\n",
    "\n",
    "For example, a GAM model for this dataset might include the following terms:\n",
    "\n",
    "*   A smooth function for the relationship between size and price, which captures the non-linear, quadratic relationship.\n",
    "*   A smooth function for the relationship between the number of bedrooms and price, which captures the non-linear, sigmoidal relationship.\n",
    "*   A smooth function for the relationship between distance and price, which captures the non-linear, exponential relationship.\n",
    "*   Interaction terms between size and bedrooms, and between distance and size, which capture the interactions between these independent variables.\n",
    "\n",
    "By interpreting the smooth functions and interaction terms in the GAM model, we can gain insights into the relationships between the independent variables and the target variable, and make more accurate predictions of house prices.\n",
    "\n",
    "For example, we might find that:\n",
    "\n",
    "*   The smooth function for size and price shows that the relationship is quadratic, with a peak at around 1500 square feet.\n",
    "*   The smooth function for bedrooms and price shows that the relationship is sigmoidal, with a steep increase in price for houses with 2-3 bedrooms, and a slower increase for houses with more bedrooms.\n",
    "*   The smooth function for distance and price shows that the relationship is exponential, with a rapid decrease in price as distance from the city center increases.\n",
    "*   The interaction term between size and bedrooms shows that the effect of size on price depends on the number of bedrooms, with larger houses having a greater effect on price for houses with more bedrooms.\n",
    "\n",
    "These insights can be useful for understanding the relationships between the independent variables and the target variable, and for making more accurate predictions of house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here's an explanation of the parameters used in the `LinearGAM` model and how to perform hyperparameter tuning:\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "1.  **`n_splines`**: This parameter controls the number of splines used to model the relationship between each feature and the target variable. A higher value of `n_splines` allows for a more complex and non-linear relationship, but may also increase the risk of overfitting.\n",
    "2.  **`lam`**: This parameter controls the regularization strength of the model. A higher value of `lam` will result in a simpler model with less overfitting, but may also reduce the model's ability to capture complex relationships.\n",
    "3.  **`intercept`**: This parameter controls whether the model includes an intercept term. If `intercept` is `True`, the model will include an intercept term, which can help to capture the overall mean of the target variable.\n",
    "\n",
    "**Hyperparameter Tuning:**\n",
    "\n",
    "Hyperparameter tuning involves searching for the optimal combination of hyperparameters that results in the best model performance. Here are some steps to perform hyperparameter tuning for the `LinearGAM` model:\n",
    "\n",
    "1.  **Define the hyperparameter space**: Define the range of values for each hyperparameter that you want to search. For example, you might want to search for `n_splines` values between 10 and 50, `lam` values between 0.1 and 10, and `intercept` values of `True` or `False`.\n",
    "2.  **Choose a search algorithm**: There are several search algorithms available, including grid search, random search, and Bayesian optimization. Grid search is a simple and exhaustive approach that involves trying all possible combinations of hyperparameters. Random search is a more efficient approach that involves randomly sampling the hyperparameter space. Bayesian optimization is a more advanced approach that involves using a probabilistic model to search for the optimal hyperparameters.\n",
    "3.  **Evaluate model performance**: Evaluate the performance of each model using a metric such as mean squared error or R2 score. You can use cross-validation to evaluate model performance on unseen data.\n",
    "4.  **Select the best model**: Select the model with the best performance based on the evaluation metric.\n",
    "\n",
    "**Example Code:**\n",
    "\n",
    "Here's an example code that performs hyperparameter tuning for the `LinearGAM` model using grid search:\n",
    "```python\n",
    "import numpy as np\n",
    "from pygam import LinearGAM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_grid = {\n",
    "    'n_splines': [10, 20, 30, 40, 50],\n",
    "    'lam': [0.1, 1, 10],\n",
    "    'intercept': [True, False]\n",
    "}\n",
    "\n",
    "# Create a LinearGAM model\n",
    "gam_model = LinearGAM(s(0) + s(1) + s(2))\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gam_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "# Train a new model with the best hyperparameters\n",
    "best_model = LinearGAM(s(0) + s(1) + s(2), **grid_search.best_params_)\n",
    "best_model.fit(X_train, y_train)\n",
    "```\n",
    "This code defines the hyperparameter space, performs grid search using `GridSearchCV`, and trains a new model with the best hyperparameters.\n",
    "\n",
    "**Random Search:**\n",
    "\n",
    "Here's an example code that performs hyperparameter tuning using random search:\n",
    "```python\n",
    "import numpy as np\n",
    "from pygam import LinearGAM\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_grid = {\n",
    "    'n_splines': [10, 20, 30, 40, 50],\n",
    "    'lam': [0.1, 1, 10],\n",
    "    'intercept': [True, False]\n",
    "}\n",
    "\n",
    "# Create a LinearGAM model\n",
    "gam_model = LinearGAM(s(0) + s(1) + s(2))\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(gam_model, param_grid, cv=5, scoring='neg_mean_squared_error', n_iter=10)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding score\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)\n",
    "print(\"Best score:\", random_search.best_score_)\n",
    "\n",
    "# Train a new model with the best hyperparameters\n",
    "best_model = LinearGAM(s(0) + s(1) + s(2), **random_search.best_params_)\n",
    "best_model.fit(X_train, y_train)\n",
    "```\n",
    "This code defines the hyperparameter space, performs random search using `RandomizedSearchCV`, and trains a new model with the best hyperparameters.\n",
    "\n",
    "**Bayesian Optimization:**\n",
    "\n",
    "Here's an example code that performs hyperparameter tuning using Bayesian optimization:\n",
    "```python\n",
    "import numpy as np\n",
    "from pygam import LinearGAM\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_grid = {\n",
    "    'n_splines': (10, 50),\n",
    "    'lam': (0.1, 10),\n",
    "    'intercept': [True, False]\n",
    "}\n",
    "\n",
    "# Create a LinearGAM model\n",
    "gam_model = LinearGAM(s(0) + s(1) + s(2))\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "bayes_search = BayesSearchCV(gam_model, param_grid, cv=5, scoring='neg_mean_squared_error', n_iter=10)\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding score\n",
    "print(\"Best hyperparameters:\", bayes_search.best_params_)\n",
    "print(\"Best score:\", bayes_search.best_score_)\n",
    "\n",
    "# Train a new model with the best hyperparameters\n",
    "best_model = LinearGAM(s(0) + s(1) + s(2), **bayes_search.best_params_)\n",
    "best_model.fit(X_train, y_train)\n",
    "```\n",
    "This code defines the hyperparameter space, performs Bayesian optimization using `BayesSearchCV`, and trains a new model with the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here's a step-by-step guide on how data scientists in corporate implement Generalized Additive Models (GAMs) in corporate projects, explaining the iterative process from end to end:\n",
    "\n",
    "**Step 1: Problem Definition and Objective Setting**\n",
    "\n",
    "* Identify a business problem or opportunity that can be addressed using GAMs, such as predicting customer churn, forecasting sales, or optimizing pricing.\n",
    "* Define the project objectives, key performance indicators (KPIs), and desired outcomes.\n",
    "* Collaborate with stakeholders to ensure everyone is aligned on the project goals and expectations.\n",
    "\n",
    "**Step 2: Data Collection and Preprocessing**\n",
    "\n",
    "* Gather relevant data from various sources, such as customer databases, transactional data, social media, or sensor data.\n",
    "* Clean, transform, and preprocess the data by handling missing values, outliers, and data normalization.\n",
    "* Feature engineering: extract relevant features from the data that can be used to build the GAM model.\n",
    "\n",
    "**Step 3: Exploratory Data Analysis (EDA)**\n",
    "\n",
    "* Perform EDA to understand the distribution of the target variable and the relationships between the features.\n",
    "* Visualize the data using plots, such as histograms, scatter plots, and correlation heatmaps.\n",
    "* Identify potential interactions between features and the target variable.\n",
    "\n",
    "**Step 4: Model Selection and Specification**\n",
    "\n",
    "* Decide on the type of GAM to use, such as a linear GAM, logistic GAM, or generalized additive mixed model (GAMM).\n",
    "* Choose the smoothing parameters, such as the number of basis functions, knots, and penalty terms.\n",
    "* Specify the model formula, including the response variable, predictor variables, and any interactions.\n",
    "\n",
    "**Step 5: Model Estimation and Fitting**\n",
    "\n",
    "* Estimate the GAM model using a suitable algorithm, such as the backfitting algorithm or the Bayesian approach.\n",
    "* Fit the model to the training data, using techniques such as cross-validation to prevent overfitting.\n",
    "* Monitor the model's performance using metrics such as mean squared error (MSE), mean absolute error (MAE), or R-squared.\n",
    "\n",
    "**Step 6: Model Evaluation and Validation**\n",
    "\n",
    "* Evaluate the model's performance on a holdout test set or using techniques such as cross-validation.\n",
    "* Validate the model by checking for:\n",
    "\t+ Overfitting or underfitting\n",
    "\t+ Model assumptions (e.g., linearity, homoscedasticity)\n",
    "\t+ Feature importance and partial dependence plots\n",
    "* Refine the model as needed, by adjusting the smoothing parameters, adding or removing features, or trying different model specifications.\n",
    "\n",
    "**Step 7: Model Interpretation and Visualization**\n",
    "\n",
    "* Interpret the model results, including the estimated coefficients, smoothing functions, and partial dependence plots.\n",
    "* Visualize the results using plots, such as:\n",
    "\t+ Partial dependence plots to show the relationship between each feature and the response variable\n",
    "\t+ Interaction plots to show the relationships between features\n",
    "\t+ Residual plots to check for model assumptions\n",
    "\n",
    "**Step 8: Model Deployment and Monitoring**\n",
    "\n",
    "* Deploy the model in a production-ready environment, using techniques such as model serving or containerization.\n",
    "* Monitor the model's performance in real-time, using metrics such as prediction accuracy, precision, and recall.\n",
    "* Continuously collect new data and retrain the model as needed to maintain its performance and adapt to changing patterns.\n",
    "\n",
    "**Step 9: Model Refining and Maintenance**\n",
    "\n",
    "* Regularly review the model's performance and refine it as needed, using techniques such as:\n",
    "\t+ Model updating: retraining the model on new data\n",
    "\t+ Model ensemble: combining multiple models to improve performance\n",
    "\t+ Model selection: choosing the best model from a set of candidates\n",
    "* Maintain the model by updating the training data, adjusting the model parameters, and ensuring the model remains relevant and effective.\n",
    "\n",
    "**Iterative Process**\n",
    "\n",
    "The process of implementing GAMs in corporate projects is iterative, with each step informing and refining the previous one. The data scientist may need to revisit earlier steps based on the results of later steps. For example:\n",
    "\n",
    "* If the model evaluation reveals poor performance, the data scientist may need to revisit the data preprocessing step to handle missing values or outliers.\n",
    "* If the model interpretation reveals unexpected relationships, the data scientist may need to revisit the feature engineering step to extract new features or transform existing ones.\n",
    "\n",
    "By following these steps and iterating through the process, data scientists can effectively implement GAMs in corporate projects, driving business value and insights through data-driven decision-making. \n",
    "\n",
    "Here is a high level overview of how GAM can be implemented in python:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pygam import LinearGAM, s, f\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the GAM model\n",
    "gam = LinearGAM(s(0) + s(1) + s(2) + s(3) + s(4) + s(5) + s(6) + s(7) + s(8) + s(9)).fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gam.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'MSE: {mse:.2f}')\n",
    "\n",
    "# Partial dependence plot for feature 0\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "xx = np.linspace(X_train.iloc[:, 0].min(), X_train.iloc[:, 0].max())\n",
    "yy = gam.partial_dependence(xx, 0)\n",
    "plt.plot(xx, yy)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Note: The above code is just an example and may need to be modified based on the actual data and problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
