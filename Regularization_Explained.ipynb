{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization Techniques: L1, L2, and Elastic Net**\n",
    "==============================================\n",
    "\n",
    "### Why Regularization?\n",
    "\n",
    "* **Prevent Overfitting**: Reduce model complexity and improve generalization\n",
    "* **Add Penalty Term**: to the loss function, proportional to the magnitude of coefficients\n",
    "\n",
    "### L1 Regularization (Lasso Regression)\n",
    "-----------------------------\n",
    "\n",
    "* **How it Works**:\n",
    "\t+ Adds term to loss function proportional to **absolute value of coefficients** (`|β|`)\n",
    "* **Effect**:\n",
    "\t+ **Feature Selection**: Sets coefficients of non-important features to zero (sparse solution)\n",
    "\t+ **Reduces Multicollinearity**: Helps when features are highly correlated\n",
    "* **When to Use**:\n",
    "\t+ **High-Dimensional Data** with many features\n",
    "\t+ **Feature Selection** is primary goal\n",
    "\t+ **Interpretable Models** are desired (due to sparse solutions)\n",
    "\n",
    "### L2 Regularization (Ridge Regression)\n",
    "-----------------------------\n",
    "\n",
    "* **How it Works**:\n",
    "\t+ Adds term to loss function proportional to **square of coefficients** (`β²`)\n",
    "* **Effect**:\n",
    "\t+ **Reduces Coefficient Magnitude**: Prevents any single feature from dominating\n",
    "\t+ **Handles Multicollinearity**: Reduces impact of correlated features\n",
    "* **When to Use**:\n",
    "\t+ **Models with Many Features** that are not too highly correlated\n",
    "\t+ **Preventing Overfitting** is primary goal\n",
    "\t+ **Non-Sparse Solutions** are acceptable\n",
    "\n",
    "### Elastic Net Regularization\n",
    "---------------------------\n",
    "\n",
    "* **How it Works**:\n",
    "\t+ Combines both L1 and L2 regularization terms\n",
    "* **Effect**:\n",
    "\t+ **Balances Feature Selection and Coefficient Reduction**\n",
    "\t+ **Handles High-Dimensional Data with Correlated Features**\n",
    "* **When to Use**:\n",
    "\t+ **High-Dimensional Data** with correlated features\n",
    "\t+ **Both Feature Selection and Preventing Overfitting** are important\n",
    "\t+ **L1 and L2 alone are not sufficient**\n",
    "\n",
    "### Hyperparameter Tuning for Regularization\n",
    "--------------------------------------\n",
    "\n",
    "* **α (alpha)**: Controls regularization strength (higher values = more regularization)\n",
    "* **λ (lambda)**: Used in some libraries to represent regularization strength (similar to α)\n",
    "* **Ratio of L1 to L2** (for Elastic Net): Adjusts balance between L1 and L2 regularization\n",
    "\n",
    "### Example Code (Python) using Scikit-learn\n",
    "```python\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# L1 Regularization (Lasso)\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "print(\"Lasso Coefficients:\", lasso_model.coef_)\n",
    "\n",
    "# L2 Regularization (Ridge)\n",
    "ridge_model = Ridge(alpha=0.1)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "print(\"Ridge Coefficients:\", ridge_model.coef_)\n",
    "\n",
    "# Elastic Net Regularization\n",
    "elastic_net_model = ElasticNet(l1_ratio=0.5, alpha=0.1)\n",
    "elastic_net_model.fit(X_train, y_train)\n",
    "print(\"Elastic Net Coefficients:\", elastic_net_model.coef_)\n",
    "```\n",
    "**Result Summary**\n",
    "\n",
    "| Regularization Technique | Description | When to Use |\n",
    "| --- | --- | --- |\n",
    "| **L1 (Lasso)** | Feature selection, sparse solution | High-dimensional data, feature selection primary goal |\n",
    "| **L2 (Ridge)** | Reduces coefficient magnitude, handles multicollinearity | Models with many features, preventing overfitting primary goal |\n",
    "| **Elastic Net** | Balances feature selection and coefficient reduction | High-dimensional data with correlated features, both feature selection and preventing overfitting important |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the Optimal Alpha Value and L1 Ratio for Regularization**\n",
    "\n",
    "**Alpha Value (α)**\n",
    "\n",
    "* **Definition:** The strength of regularization, where:\n",
    "\t+ **High α**: Stronger regularization, simpler models\n",
    "\t+ **Low α**: Weaker regularization, more complex models\n",
    "* **Methods to Find the Optimal Alpha Value:**\n",
    "\n",
    "1. ****Grid Search****:\n",
    "\t* Try a range of α values (e.g., `[0.01, 0.1, 1, 10]`)\n",
    "\t* Evaluate the model's performance for each α using a metric (e.g., cross-validation score)\n",
    "\t* Select the α with the best performance\n",
    "2. ****Random Search****:\n",
    "\t* Similar to grid search, but α values are randomly sampled from a distribution\n",
    "\t* Can be more efficient than grid search for large hyperparameter spaces\n",
    "3. ****Cross-Validation****:\n",
    "\t* Split data into training and validation sets\n",
    "\t* Perform grid search or random search on the training set\n",
    "\t* Evaluate the best α on the validation set\n",
    "4. ****Learning Curve****:\n",
    "\t* Plot the model's performance against different α values\n",
    "\t* Visualize the trade-off between underfitting and overfitting\n",
    "\n",
    "**L1 Ratio (for Elastic Net)**\n",
    "\n",
    "* **Definition:** The balance between L1 and L2 regularization, where:\n",
    "\t+ **L1 Ratio = 0**: Equivalent to L2 regularization (Ridge)\n",
    "\t+ **L1 Ratio = 1**: Equivalent to L1 regularization (Lasso)\n",
    "\t+ **0 < L1 Ratio < 1**: Combination of L1 and L2 regularization (Elastic Net)\n",
    "* **Methods to Find the Optimal L1 Ratio:**\n",
    "\n",
    "1. ****Grid Search****:\n",
    "\t* Try a range of L1 Ratio values (e.g., `[0, 0.2, 0.5, 0.8, 1]`)\n",
    "\t* Evaluate the model's performance for each L1 Ratio using a metric (e.g., cross-validation score)\n",
    "\t* Select the L1 Ratio with the best performance\n",
    "2. ****Random Search****:\n",
    "\t* Similar to grid search, but L1 Ratio values are randomly sampled from a distribution\n",
    "3. ****Cross-Validation****:\n",
    "\t* Split data into training and validation sets\n",
    "\t* Perform grid search or random search on the training set\n",
    "\t* Evaluate the best L1 Ratio on the validation set\n",
    "\n",
    "**Example Code (Python) for Grid Search and Cross-Validation**\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 1, 10],\n",
    "    'l1_ratio': [0, 0.2, 0.5, 0.8, 1]\n",
    "}\n",
    "\n",
    "# Initialize the Elastic Net model\n",
    "model = ElasticNet()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "```\n",
    "**Tips and Variations**\n",
    "\n",
    "* **Use a validation set** to evaluate the best hyperparameters and avoid overfitting.\n",
    "* **Try different hyperparameter ranges** and distributions (e.g., logarithmic scale for α).\n",
    "* **Use random search** for large hyperparameter spaces or when computational resources are limited.\n",
    "* **Visualize the learning curve** to understand the trade-off between underfitting and overfitting.\n",
    "* **Consider using Bayesian optimization** for more efficient hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's consider a simple example to understand how adding a penalty term to the loss function works.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Suppose we have a linear regression model that predicts the price of a house based on its size. The model is trained on a dataset of houses with their sizes and prices.\n",
    "\n",
    "**Loss Function:**\n",
    "\n",
    "The loss function for linear regression is typically the Mean Squared Error (MSE), which measures the average squared difference between the predicted prices and the actual prices.\n",
    "\n",
    "MSE = (1/n) \\* Σ(y_true - y_pred)^2\n",
    "\n",
    "where y_true is the actual price, y_pred is the predicted price, and n is the number of data points.\n",
    "\n",
    "**Adding a Penalty Term:**\n",
    "\n",
    "Now, let's say we want to add a penalty term to the loss function to prevent the model from overfitting. We can add a term that penalizes the model for having large weights.\n",
    "\n",
    "The updated loss function would be:\n",
    "\n",
    "Loss = MSE + α \\* ||w||^2\n",
    "\n",
    "where w is the weight vector, α is the regularization strength, and ||.||^2 is the L2 norm (sum of squares of the weights).\n",
    "\n",
    "**How it Works:**\n",
    "\n",
    "When we add the penalty term to the loss function, the model is no longer just trying to minimize the MSE. It's also trying to minimize the penalty term, which is proportional to the size of the weights.\n",
    "\n",
    "The α parameter controls the strength of the penalty term. If α is small, the penalty term has little effect, and the model will still try to fit the data closely. If α is large, the penalty term has a significant effect, and the model will be more conservative in its predictions.\n",
    "\n",
    "**Example Calculation:**\n",
    "\n",
    "Let's say we have a dataset of 5 houses with sizes and prices:\n",
    "\n",
    "| Size | Price |\n",
    "| --- | --- |\n",
    "| 1000 | 200000 |\n",
    "| 1200 | 250000 |\n",
    "| 1500 | 300000 |\n",
    "| 1800 | 350000 |\n",
    "| 2000 | 400000 |\n",
    "\n",
    "We train a linear regression model on this data, and the model learns the following weights:\n",
    "\n",
    "w = [0.5, 0.2]\n",
    "\n",
    "The predicted prices are:\n",
    "\n",
    "| Size | Predicted Price |\n",
    "| --- | --- |\n",
    "| 1000 | 180000 |\n",
    "| 1200 | 220000 |\n",
    "| 1500 | 280000 |\n",
    "| 1800 | 340000 |\n",
    "| 2000 | 400000 |\n",
    "\n",
    "The MSE is:\n",
    "\n",
    "MSE = (1/5) \\* (200000 - 180000)^2 + (250000 - 220000)^2 +... = 1000000\n",
    "\n",
    "The penalty term is:\n",
    "\n",
    "α \\* ||w||^2 = 0.1 \\* (0.5^2 + 0.2^2) = 0.03\n",
    "\n",
    "The updated loss function is:\n",
    "\n",
    "Loss = MSE + α \\* ||w||^2 = 1000000 + 0.03 = 1000003\n",
    "\n",
    "**Effect of Penalty Term:**\n",
    "\n",
    "If we increase the value of α, the penalty term will become larger, and the model will be more conservative in its predictions. For example, if α = 1, the penalty term would be:\n",
    "\n",
    "α \\* ||w||^2 = 1 \\* (0.5^2 + 0.2^2) = 0.29\n",
    "\n",
    "The updated loss function would be:\n",
    "\n",
    "Loss = MSE + α \\* ||w||^2 = 1000000 + 0.29 = 1000029\n",
    "\n",
    "In this case, the model would prefer to have smaller weights, which would result in more conservative predictions.\n",
    "\n",
    "I hope this example helps illustrate how adding a penalty term to the loss function works! Let me know if you have any questions or need further clarification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
